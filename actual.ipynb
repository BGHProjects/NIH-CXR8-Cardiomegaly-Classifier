{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH CXR8 Classifier\n",
    "The following is a machine learning exercise on constructing a model that can detect cases of Cardiomegaly from the NIH CXR8 dataset and evaluating the effectiveness of the model. The following code is made for Google Collab, and may not necessarily run correctly as a simple Python Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant libraries and dependencies\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "repo_url = 'https://github.com/adleberg/medical-ai'\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 256, 256\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    image = image.convert('RGB')\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "print(\"Welcome! Downloading some things... this will take a minute.\")\n",
    "\n",
    "%cd -q /content\n",
    "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
    "!git clone {repo_url} --quiet\n",
    "%cd -q {repo_dir_path}\n",
    "!git pull -q\n",
    "\n",
    "print(\"Great! You clicked on it correctly. Now let's get started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data\n",
    "The dataset is going to split into training data, which will represent 80% of the corpus, and testing data. In order to perform the classification, the dataset will also be split into negative and positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding = \"cardiomegaly\"\n",
    "finding = finding.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/medical-ai/labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = df.loc[df[\"label\"] == finding]\n",
    "negatives = df.loc[df[\"label\"] == \"No Finding\"]\n",
    "n = len(positives)\n",
    "\n",
    "if n == 0:\n",
    "  print(\"No studies found! Maybe check your spelling?\")\n",
    "  assert (n > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 1 - TRAIN_RATIO\n",
    "n = len(positives)\n",
    "TRAIN_N = int(n*TRAIN_RATIO)\n",
    "TEST_N = int(n*TEST_RATIO)\n",
    "print(TRAIN_N, TEST_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.concat([positives[:TRAIN_N], negatives[:TRAIN_N]])\n",
    "test_labels = pd.concat([positives[TRAIN_N:], negatives[TRAIN_N:n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"/content/medical-ai/images/\"\n",
    "\n",
    "dirs = [\"/test/positive\", \"/test/negative\", \"/train/positive\", \"/train/negative\" ]\n",
    "\n",
    "for dir in dirs:\n",
    "  os.makedirs(rootdir+finding+dir,  exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy images to new directories for training purposes\n",
    "\n",
    "image_sets = [ (positives[:TRAIN_N], \"/train/positive/\"), (positives[TRAIN_N:],\"/test/positive/\"), (negatives[:TRAIN_N], \"/train/negative/\"), (negatives[TRAIN_N:n],\"/test/negative/\" )]\n",
    "\n",
    "for image_set in image_sets:\n",
    "  for idx, image in image_set[0].iterrows():\n",
    "    source = rootdir+image[\"filename\"]\n",
    "    dst = rootdir+finding+image_set[1]+image[\"filename\"]\n",
    "    shutil.copy(source, dst)\n",
    "\n",
    "print(\"Done moving \"+str(n*2)+\" images to positive and negative folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into memory for visualization\n",
    "positive_imgs, negative_imgs = [], []\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 256, 256\n",
    "image_set_end = 6\n",
    "\n",
    "image_sets = [(positives, positive_imgs), (negatives, negative_imgs)]\n",
    "\n",
    "for image_set in image_sets:\n",
    "  for idx, row in image_set[0][:image_set_end].iterrows():\n",
    "    image_path = rootdir+row[\"filename\"]\n",
    "    image = Image.open(image_path).resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    image_set[1].append(load_image_into_numpy_array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sets = [positive_imgs, negative_imgs]\n",
    "\n",
    "for index, image_set in enumerate(image_sets):\n",
    "  for idx, img in enumerate(image_set[:6]):\n",
    "    plt.subplot(2, 3, idx+1)\n",
    "    plt.title(finding if index == 0 else \"No Findings\")\n",
    "    plt.imshow(image_set[idx])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training the Model\n",
    "\n",
    "The model used is InceptionV3, with a modified final few layers that involve flattening the model to 1 dimension, adding a 20% dropout, and applying Sigmoid activation for the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = InceptionV3(\n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = layers.Flatten()(last_output) # Flatten the output layer to 1 dimension\n",
    "x = layers.Dense(1024, activation='relu')(x) # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "x = layers.Dropout(0.2)(x) # Add a dropout rate of 0.2\n",
    "x = layers.Dense(1, activation='sigmoid')(x) # Add a final sigmoid layer for classification\n",
    "\n",
    "model = Model(pre_trained_model.input, x) # Configure and compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(\"Done compiling the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our example directories and files\n",
    "base_dir = rootdir = \"/content/medical-ai/images/\"\n",
    "train_dir = os.path.join(base_dir, finding, 'train')\n",
    "test_dir = os.path.join(base_dir, finding, 'test')\n",
    "\n",
    "train_pos_dir = os.path.join(train_dir, 'positive')\n",
    "train_neg_dir = os.path.join(train_dir, 'negative')\n",
    "test_pos_dir = os.path.join(test_dir, 'positive')\n",
    "test_neg_dir = os.path.join(test_dir, 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "# Note that the test data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, # This is the source directory for training images\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=1,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=1,\n",
    "        class_mode='binary')\n",
    "\n",
    "train_steps = len(os.listdir(train_pos_dir)) * 2\n",
    "test_steps = len(os.listdir(test_pos_dir)) * 2\n",
    "\n",
    "print(train_steps)\n",
    "print(test_steps)\n",
    "\n",
    "print(\"Done funneling data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the Model\n",
    "Now that the model has been trained, it will be tested on the test dataset, and its accuracy and loss will be evaluated graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_steps,\n",
    "      epochs=20,\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=test_steps,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of accuracy results on training and test data\n",
    "# sets for each training epoch\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "graph_vals = [(acc, val_acc, \"Accuracy\"), (loss, val_loss, \"Loss\")]\n",
    "\n",
    "for index, graph_val in enumerate(graph_vals):\n",
    "  plt.subplot(2,1,index + 1)\n",
    "  plt.plot(epochs, graph_val[0], label=\"train\")\n",
    "  plt.plot(epochs, graph_val[1], label=\"test\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(graph_val[2])\n",
    "  plt.title('Training and test ' + graph_val[2])\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating the Model\n",
    "In this section, the model will be evaluted more deeply to observe its performance using a histogram, and evaluating its specificity, sensitivity, and receiver operating characteristic (or area under the curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(filename):\n",
    "  image = Image.open(filename).resize((IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  exp = np.true_divide(image_np, 255.0)\n",
    "  expanded = np.expand_dims(exp, axis=0)\n",
    "  return model.predict(expanded)[0][0]\n",
    "\n",
    "def show_df_row(row):\n",
    "  image_path = row[\"filepath\"]\n",
    "  image = Image.open(image_path).resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "  img = load_image_into_numpy_array(image)\n",
    "  exp = np.true_divide(img, 255.0)\n",
    "  expanded = np.expand_dims(exp, axis=0)\n",
    "  pred = model.predict(expanded)[0][0]\n",
    "  guess = \"neg\"\n",
    "  if pred > 0.5:\n",
    "    guess = \"pos\"\n",
    "  title = \"Image: \"+row[\"filename\"]+\" Label: \"+row[\"label\"]+\" Guess: \"+guess+\" Score: \"+str(pred)\n",
    "  plt.title(title)\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "image_vals = [(test_neg_dir, \"neg\"), (test_pos_dir, \"pos\")]\n",
    "\n",
    "for image_val in image_vals:\n",
    "  for image in os.listdir(image_val[0]):\n",
    "    filename = image_val[0]+\"/\"+image\n",
    "    confidence = predict_image(filename)\n",
    "    guess = 'pos' if confidence > 0.5 else 'neg'\n",
    "    results.append([filename, image, image_val[1], guess, confidence])\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x[4], reverse=True)\n",
    "df = pd.DataFrame(data=sorted_results, columns=[\"filepath\",\"filename\",\"label\",\"guess\",\"confidence\"])\n",
    "\n",
    "print(\"Done inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example image\n",
    "import random\n",
    "n = random.randint(0, len(df)-1)\n",
    "show_df_row(df.iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a table of images\n",
    "df[::5][['filename', 'label',\"guess\",\"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histogram\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "pos = df.loc[df['label'] == \"pos\"][\"confidence\"]\n",
    "neg = df.loc[df['label'] == \"neg\"][\"confidence\"]\n",
    "fig, ax = plt.subplots()\n",
    "n, bins, patches = plt.hist([pos,neg], np.arange(0.0, 1.1, 0.1).tolist(), edgecolor='black', linewidth=0.5, density=False, histtype='bar', stacked=True, color=['green', 'red'], label=[finding, 'Negative'])\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('N')\n",
    "plt.xticks(bins)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.title('Confidence scores for different values')\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity / Sensitvity Cutoff\n",
    "\n",
    "cutoff = 0.79 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "\n",
    "def create_with_cutoff(cutoff):\n",
    "  __, ax = plt.subplots()\n",
    "  TP = df.loc[(df['label'] == \"pos\") & (df[\"confidence\"] > cutoff)][\"confidence\"]\n",
    "  FP = df.loc[(df['label'] == \"neg\") & (df[\"confidence\"] > cutoff)][\"confidence\"]\n",
    "  FN = df.loc[(df['label'] == \"pos\") & (df[\"confidence\"] < cutoff)][\"confidence\"]\n",
    "  TN = df.loc[(df['label'] == \"neg\") & (df[\"confidence\"] < cutoff)][\"confidence\"]\n",
    "  plt.hist([TP,FP,TN,FN], np.arange(0.0, 1.1, 0.1).tolist(), \\\n",
    "           edgecolor='black', linewidth=0.5, density=False, histtype='bar', \\\n",
    "           stacked=True, color=['limegreen','forestgreen','orangered','salmon'], \\\n",
    "           label=['TP','FP','TN','FN'])\n",
    "  plt.xlabel('Confidence')\n",
    "  plt.ylabel('N')\n",
    "  plt.xticks(bins)\n",
    "  ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "  plt.title('Confidence scores for different values')\n",
    "  plt.axvline(cutoff, color='k', linestyle='dashed', linewidth=2)\n",
    "  plt.legend(loc=\"lower right\", fontsize=16)\n",
    "  sens = round(len(TP)/(len(TP)+len(FN)),2)\n",
    "  spec = round(len(TN)/(len(TN)+len(FP)),2)\n",
    "  stats = \"sensitivity: \"+str(sens)+\"\\n\"+\"specificity: \"+str(spec)+\"\\n\\n\"+\"TP: \"+str(len(TP))+\"\\n\"+\"FP: \"+str(len(FP))+\"\\n\"+\"TN: \"+str(len(TN))+\"\\n\"+\"FN: \"+str(len(FN))\n",
    "  plt.text(0.05, 0.05, stats, fontsize=14, transform=ax.transAxes)\n",
    "  plt.show()\n",
    "\n",
    "create_with_cutoff(cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "def create_auc_curve(classifications):\n",
    "  squares = {}\n",
    "  for x in classifications:\n",
    "    conf = x[4]\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for row in classifications:\n",
    "      assert (row[2] == \"neg\" or row[2] == \"pos\")\n",
    "      if row[2] == \"neg\":\n",
    "        if float(row[4]) < conf: TN += 1\n",
    "        else: FP += 1\n",
    "      else:\n",
    "        if float(row[4]) > conf: TP += 1\n",
    "        else: FN += 1\n",
    "    squares[conf] = [TP, FP, TN, FN]\n",
    "  # now we have a list of stuff: convert to\n",
    "  sens_spec = {}\n",
    "  for entry in squares:\n",
    "    sens = squares[entry][0] / float(squares[entry][0] + squares[entry][3])\n",
    "    spec = squares[entry][2] / float(squares[entry][2] + squares[entry][1])\n",
    "    sens_spec[entry] = (1-spec, sens)\n",
    "  return squares, sens_spec\n",
    "\n",
    "squares, sens_spec = create_auc_curve(sorted_results)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for point in sens_spec.keys():\n",
    "  x.append(sens_spec[point][0])\n",
    "  y.append(sens_spec[point][1])\n",
    "\n",
    "auc = sklearn.metrics.auc(x, y)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(x, y, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.xlabel('1-specificity')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the Model\n",
    "The following code allows you to save the model from Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/content/export/'+finding)\n",
    "!zip -r /content/{finding}.zip /content/export/{finding}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
